{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad46e1df-7a97-4209-b5e4-e4b63bfa915d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shoaib.rahman/.pyenv/versions/3.13.3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sieve-GNN: Pipeline Start (11 Workers) ---\n",
      "Phase 1: Profiling 4431 tables...\n",
      "  Processed 4400/4431...\n",
      "  > Candidates: 341810\n",
      "Phase 2: Building Graph...\n",
      "\n",
      "Phase 3: GNN Training (150 Epochs)...\n",
      "Epoch  | Loss     | Acc    | Prec   | Rec    | F1    \n",
      "-------------------------------------------------------\n",
      "0      | 0.5981   | 0.83   | 0.91   | 0.90   | 0.90\n",
      "25     | 0.2269   | 0.99   | 0.98   | 1.00   | 0.99\n",
      "50     | 0.2001   | 0.99   | 0.99   | 1.00   | 0.99\n",
      "75     | 0.1756   | 0.99   | 0.99   | 1.00   | 0.99\n",
      "100    | 0.1558   | 0.99   | 0.99   | 1.00   | 0.99\n",
      "125    | 0.1410   | 0.99   | 0.99   | 1.00   | 0.99\n",
      "149    | 0.1319   | 0.99   | 0.99   | 1.00   | 0.99\n",
      "-------------------------------------------------------\n",
      "Phase 4: Generating JSON & Data Quality Profile...\n",
      "  > Saved: data_quality_profile.csv\n",
      "\n",
      "Phase 5: Refinement & Exporting...\n",
      "  > Analyzing graph edges for Foreign Keys...\n",
      "  > Note: 246 pairs had some overlap (>30%) but failed validation.\n",
      "  > Saved: relationships.csv (13 relations found)\n",
      "  > Drawing ERD (Connected Tables Only)...\n",
      "  > Saved: erd.png\n",
      "  > Saved: schema_final.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import importlib.util\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "##### ==========================================#####\n",
    "##### WORKER CODE (for parallel processing)#####\n",
    "##### ==========================================#####\n",
    "WORKER_CODE = \"\"\"\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "\n",
    "##### Normalization: Type standardization & warning suppression#####\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "\n",
    "SAMPLE_SIZE = 500\n",
    "SIGNATURE_SIZE = 200\n",
    "\n",
    "def smart_normalize(series):\n",
    "    ##### Normalization: Trim, casefold, numeric parsing\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    try:\n",
    "        nums = pd.to_numeric(s, errors='coerce')\n",
    "        mask = nums.notna()\n",
    "        if mask.mean() > 0.8: \n",
    "            s.loc[mask] = nums.loc[mask].astype(float).astype(str)\n",
    "    except: pass\n",
    "    return s\n",
    "\n",
    "def get_id_score(col_name):\n",
    "######################### Node Features: ID-like tokens #########################\n",
    "    col = col_name.lower()\n",
    "    if col in ['id', 'pk', 'key', 'uuid', 'guid', 'index', '_id']: return 3\n",
    "    if re.search(r'(_id|id|_key|key|_pk|pk)$', col): return 2\n",
    "    if re.search(r'(id|key|code|num|no)', col): return 1\n",
    "    return 0\n",
    "\n",
    "def get_signature(series, n=200):\n",
    "    #####  Blocking LSH: MinHash-style signature for fast comparison\n",
    "    uniques = series.dropna().unique()\n",
    "    if len(uniques) > n:\n",
    "        try: uniques = np.random.choice(uniques, n, replace=False)\n",
    "        except: pass\n",
    "    return set(hash(str(x)) for x in uniques)\n",
    "\n",
    "def extract_features(df, cols):\n",
    "    is_composite = isinstance(cols, tuple)\n",
    "    col_names = list(cols) if is_composite else [cols]\n",
    "    n_rows = len(df)\n",
    "    \n",
    "##### Robustness : Multiple row samples (Head, Tail, Random)#########################\n",
    "    limit = SAMPLE_SIZE\n",
    "    try:\n",
    "        head = df[col_names].head(limit)\n",
    "        tail = df[col_names].tail(limit)\n",
    "        rand = df[col_names].sample(min(n_rows, limit)) if n_rows > limit else df[col_names]\n",
    "    except: return None \n",
    "\n",
    "    if is_composite:\n",
    "        fn = lambda x: ''.join(x.astype(str))\n",
    "        vals_head = head.agg(fn, axis=1)\n",
    "        vals_tail = tail.agg(fn, axis=1)\n",
    "        vals_rand = rand.agg(fn, axis=1)\n",
    "        name_str = \"_\".join(cols).lower()\n",
    "        col_count = len(cols)\n",
    "        id_score = max([get_id_score(c) for c in cols])\n",
    "    else:\n",
    "        vals_head = smart_normalize(head[cols])\n",
    "        vals_tail = smart_normalize(tail[cols])\n",
    "        vals_rand = smart_normalize(rand[cols])\n",
    "        name_str = cols.lower()\n",
    "        col_count = 1\n",
    "        id_score = get_id_score(cols)\n",
    "\n",
    "##### Stability Check: Key must be stable across all samples ###################\n",
    "    u_scores = []\n",
    "    for v in [vals_head, vals_tail, vals_rand]:\n",
    "        u_scores.append(v.nunique() / len(v) if len(v) > 0 else 0)\n",
    "        \n",
    "    if (max(u_scores) - min(u_scores)) > 0.5: return None \n",
    "\n",
    "##### Node Features: Entropy, UUID regex, Nulls, Duplicates #########\n",
    "    vals = vals_rand\n",
    "    n_uniq = vals.nunique()\n",
    "    uniqueness = u_scores[2]\n",
    "    null_rate = df[col_names].isna().mean().max()\n",
    "    dup_rate = 1.0 - uniqueness\n",
    "    card_log = np.log1p(n_uniq)\n",
    "    \n",
    "    counts = vals.value_counts()\n",
    "    ent = entropy(counts) if len(counts) > 0 else 0\n",
    "    \n",
    "    is_num = 0.0\n",
    "    if not is_composite:\n",
    "        try: is_num = pd.to_numeric(vals, errors='coerce').notna().mean()\n",
    "        except: pass\n",
    "        \n",
    "    avg_len = vals.str.len().mean() if len(vals) > 0 else 0.0\n",
    "    is_uuid = 1.0 if re.search(r'(uuid|guid)', name_str) else 0.0\n",
    "    is_time = 1.0 if re.search(r'(date|time|created)', name_str) else 0.0\n",
    "\n",
    "##### 11-DIM FEATURE VECTOR #########################\n",
    "    feats = [uniqueness, null_rate, dup_rate, card_log, ent,\n",
    "             is_num, avg_len, float(id_score), is_uuid, is_time, float(col_count)]\n",
    "             \n",
    "    return feats, get_signature(vals, SIGNATURE_SIZE), avg_len, n_rows\n",
    "\n",
    "def process_table_file(filepath):\n",
    "    try:\n",
    "##### Optimization: Read once, low_memory=False #########################\n",
    "        try: df = pd.read_csv(filepath, on_bad_lines='skip', engine='pyarrow')\n",
    "        except: df = pd.read_csv(filepath, on_bad_lines='skip', low_memory=False)\n",
    "        \n",
    "        if df.empty: return []\n",
    "        \n",
    "        cols = [c for c in df.columns if not re.search(r'^(desc|note|comment|text)', c, re.I)]\n",
    "        table = filepath.split('/')[-1].replace('.csv','')\n",
    "        \n",
    "        candidates = []\n",
    "        ingredients = []\n",
    "        \n",
    "##### 1. Atomic Columns #########################\n",
    "        for col in cols:\n",
    "            res = extract_features(df, col)\n",
    "            if res:\n",
    "                feats, sig, avg_len, n_rows = res\n",
    "                candidates.append({\n",
    "                    'table': table, 'cols': (col,), \n",
    "                    'features': feats, 'signature': sig, \n",
    "                    'avg_len': avg_len, 'n_rows': n_rows\n",
    "                })\n",
    "                if feats[0] > 0.2 or feats[7] > 0: ingredients.append(col)\n",
    "        \n",
    " ##### Composite Pruning: Use top-m ingredients #########################\n",
    "        import itertools\n",
    "        if len(ingredients) > 10: ingredients = ingredients[:10]\n",
    "        \n",
    "        for r in range(2, 4):\n",
    "            for c_cols in itertools.combinations(ingredients, r):\n",
    "                res = extract_features(df, c_cols)\n",
    "                if res:\n",
    "                    feats, _, _, n_rows = res\n",
    "                    ##### [REQ-7] Composite Pruning: Uniqueness threshold\n",
    "                    if feats[0] > 0.85:\n",
    "                        candidates.append({\n",
    "                            'table': table, 'cols': c_cols, \n",
    "                            'features': feats, 'signature': set(), \n",
    "                            'avg_len': 0, 'n_rows': n_rows\n",
    "                        })\n",
    "        return candidates\n",
    "    except: return []\n",
    "\"\"\"\n",
    "\n",
    "CONFIG = {\n",
    "    'folder': './data', \n",
    "    'device': 'cpu',\n",
    "    'tau': 0.25,          ##### Lowered from 0.40 to catch weaker signals\n",
    "    'min_overlap': 2,     ##### Lowered from 3 to allow smaller overlaps\n",
    "    'num_workers': max(1, (os.cpu_count() or 4) - 1),\n",
    "    'hidden_dim': 64, 'heads': 4, 'dropout': 0.1,\n",
    "    'lr': 0.005, 'weight_decay': 1e-4, 'epochs': 150,\n",
    "}\n",
    "\n",
    "##### ==========================================#########################\n",
    "##### 2. SEMANTIC & REFINEMENT ##########\n",
    "##### ==========================================#########################\n",
    "def get_name_similarity(name1, name2):\n",
    "    n1, n2 = name1.lower(), name2.lower()\n",
    "    if n1 == n2: return 1.0\n",
    "    if n1 + 's' == n2 or n1 == n2 + 's': return 0.9 \n",
    "    if n1.replace('_id', '') == n2 or n2.replace('_id', '') == n1: return 0.8\n",
    "    return 0.0\n",
    "\n",
    "def refine_and_export(candidates, schema, data):\n",
    "    print(\"\\nPhase 5: Refinement & Exporting...\")\n",
    "    \n",
    "##### Headless backend ######################\n",
    "    \n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    edge_attr = data.edge_attr.cpu().numpy()\n",
    "    relationships = []\n",
    "    \n",
    "    print(\"  > Analyzing graph edges for Foreign Keys...\")\n",
    "##### Track \"Near Misses\" for debugging ############\n",
    "    near_misses = 0\n",
    "    \n",
    "    for k in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, k], edge_index[1, k]\n",
    "        c_src, c_dst = candidates[src], candidates[dst]\n",
    "        \n",
    "        if c_src['table'] == c_dst['table']: continue\n",
    "        \n",
    "##### Metrics ########### \n",
    "        inc_score = edge_attr[k][0]   ##### How much of Src is in Dst? ###\n",
    "        name_score = get_name_similarity(c_src['cols'][0], c_dst['table'])\n",
    "        \n",
    "        ##### LOGIC: ########\n",
    "        ##### 1. Strong Structural: 98% overlap (almost perfect subset) -> Trust it\n",
    "        ##### 2. Hybrid: 50% overlap + Name match (e.g. 'user_id' -> 'users')\n",
    "        is_strong_structural = (inc_score > 0.98)\n",
    "        is_semantic_match = (inc_score > 0.50 and name_score > 0.5)\n",
    "        \n",
    "        if is_strong_structural or is_semantic_match:\n",
    "            rel = {\n",
    "                \"Source Table\": c_src['table'], \n",
    "                \"Source Col\": list(c_src['cols']),\n",
    "                \"Target Table\": c_dst['table'], \n",
    "                \"Confidence\": f\"{inc_score:.2f}\"\n",
    "            }\n",
    "            ##### Deduplicate ###########\n",
    "            if not any(r['Source Table'] == rel['Source Table'] and r['Target Table'] == rel['Target Table'] for r in relationships):\n",
    "                relationships.append(rel)\n",
    "        elif inc_score > 0.3:\n",
    "            near_misses += 1\n",
    "\n",
    "    if near_misses > 0:\n",
    "        print(f\"  > Note: {near_misses} pairs had some overlap (>30%) but failed validation.\")\n",
    "\n",
    "##### 3. Export CSV REl #################\n",
    "    if relationships:\n",
    "        pd.DataFrame(relationships).to_csv(\"relationships.csv\", index=False)\n",
    "        print(f\"  > Saved: relationships.csv ({len(relationships)} relations found)\")\n",
    "    else:\n",
    "        print(\"  > No strong FK relationships detected.\")\n",
    "\n",
    "##### Make ERD (Connected Components sample) ######\n",
    "    try:\n",
    "        if not relationships:\n",
    "            print(\"  > Skipping ERD (No relationships to draw).\")\n",
    "            return schema\n",
    "\n",
    "        print(\"  > Drawing ERD (Connected Tables Only)...\")\n",
    "        G_viz = nx.DiGraph()\n",
    "        \n",
    "            # ONLY add nodes that exist in a relationship\n",
    "        for r in relationships:\n",
    "            G_viz.add_edge(r['Source Table'], r['Target Table'], label=\"FK\")\n",
    "        \n",
    "        plt.figure(figsize=(16, 12)) #canvasLarger\n",
    "        \n",
    "        #layout that separates clusters\n",
    "        pos = nx.spring_layout(G_viz, k=0.3, iterations=50, seed=42)\n",
    "        \n",
    "        ##drawing with high contrast\n",
    "        nx.draw_networkx_nodes(G_viz, pos, node_size=2500, node_color='#E0F7FA', edgecolors='#006064')\n",
    "        nx.draw_networkx_labels(G_viz, pos, font_size=8, font_weight='bold')\n",
    "        nx.draw_networkx_edges(G_viz, pos, edge_color='#455A64', arrowstyle='-|>', arrowsize=15, width=1.2)\n",
    "            \n",
    "        plt.title(f\"Inferred Schema ({len(relationships)} Relations)\", fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"erd.png\", dpi=300)\n",
    "        plt.close()\n",
    "        print(\"  > Saved: erd.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  > Error drawing ERD: {e}\")\n",
    "    \n",
    "    return schema\n",
    "\n",
    "##### ==========================================###############\n",
    "##### 3. GNN MODEL & GRAPH                          #####\n",
    "##### ==========================================##########\n",
    "class SieveGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, heads, dropout):\n",
    "        super().__init__()\n",
    "##### Edge Features consumed here (edge_dim=4) ##########\n",
    "        self.conv1 = GATv2Conv(in_dim, hidden_dim, heads=heads, edge_dim=4, dropout=dropout)\n",
    "        self.conv2 = GATv2Conv(hidden_dim*heads, 1, heads=1, edge_dim=4, dropout=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def build_graph_fast(candidates):\n",
    "    print(\"Phase 2: Building Graph...\")\n",
    "    ##### Blocking / LSH for speed #####\n",
    "    index = defaultdict(list)\n",
    "    for idx, c in enumerate(candidates):\n",
    "        if not c['signature']: continue\n",
    "        for token in c['signature']: index[token].append(idx)\n",
    "            \n",
    "    pair_counts = defaultdict(int)\n",
    "    for token, indices in index.items():\n",
    "        if len(indices) < 2 or len(indices) > 50: continue\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i+1, len(indices)):\n",
    "                idx_a, idx_b = indices[i], indices[j]\n",
    "                if candidates[idx_a]['table'] == candidates[idx_b]['table']: continue\n",
    "                pair = tuple(sorted((idx_a, idx_b)))\n",
    "                pair_counts[pair] += 1\n",
    "                \n",
    "    edge_index, edge_attr = [], []\n",
    "    for (i, j), overlap in pair_counts.items():\n",
    "        if overlap < CONFIG['min_overlap']: continue\n",
    "        c_i, c_j = candidates[i], candidates[j]\n",
    "        len_i, len_j = len(c_i['signature']), len(c_j['signature'])\n",
    "        \n",
    "        ## Directional Inclusion metrics\n",
    "        inc_i_j = overlap / len_i if len_i else 0\n",
    "        inc_j_i = overlap / len_j if len_j else 0\n",
    "        ####Alternative Similarity: Jaccard\n",
    "        jaccard = overlap / (len_i + len_j - overlap)\n",
    "        ### Edge Features: Value-Length Compatibility\n",
    "        li, lj = c_i['avg_len'], c_j['avg_len']\n",
    "        len_ratio = min(li, lj) / (max(li, lj) + 0.01)\n",
    "        \n",
    "        if max(inc_i_j, inc_j_i) > CONFIG['tau']:\n",
    "            edge_index.append([i, j]); edge_attr.append([inc_i_j, inc_j_i, jaccard, len_ratio])\n",
    "            edge_index.append([j, i]); edge_attr.append([inc_j_i, inc_i_j, jaccard, len_ratio])\n",
    "\n",
    "    x = torch.tensor([c['features'] for c in candidates], dtype=torch.float).nan_to_num()\n",
    "    \n",
    "    if not edge_index:\n",
    "        edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        edge_attr = torch.zeros((1, 4), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def get_weak_labels(candidates):\n",
    "    ##-- Weak/Heuristic Labels\n",
    "    labels, mask = [], []\n",
    "    for c in candidates:\n",
    "        u = c['features'][0]\n",
    "        is_composite = c['features'][10] > 1\n",
    "        if is_composite and u > 0.99: labels.append(1.0); mask.append(True)\n",
    "        elif u < 0.90: labels.append(0.0); mask.append(True)\n",
    "        else: labels.append(0.0); mask.append(False)\n",
    "    return torch.tensor(labels).float(), torch.tensor(mask).bool()\n",
    "\n",
    "def train_model(model, data, labels, mask, cfg):\n",
    "    ### Optimize with NLL, Report Acc/Prec/Recall ##### ##########\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([3.0]))\n",
    "    model.train()\n",
    "    print(f\"\\nPhase 3: GNN Training ({cfg['epochs']} Epochs)...\")\n",
    "    print(f\"{'Epoch':<6} | {'Loss':<8} | {'Acc':<6} | {'Prec':<6} | {'Rec':<6} | {'F1':<6}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for epoch in range(cfg['epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[mask].squeeze(), labels[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 25 == 0 or epoch == cfg['epochs'] - 1:\n",
    "            preds = (torch.sigmoid(out[mask]).squeeze() > 0.5).float()\n",
    "            y_true, y_pred = labels[mask].cpu().numpy(), preds.cpu().numpy()\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            print(f\"{epoch:<6} | {loss.item():.4f}   | {acc:.2f}   | {prec:.2f}   | {rec:.2f}   | {f1:.2f}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "def generate_reports(candidates, probs):\n",
    "    print(\"Phase 4: Generating JSON & Data Quality Profile...\")\n",
    "    schema = {\"tables\": {}}\n",
    "    table_groups = defaultdict(list)\n",
    "    for i, c in enumerate(candidates):\n",
    "        table_groups[c['table']].append((probs[i], c))\n",
    "\n",
    "    for table, items in table_groups.items():\n",
    "        valid = [x for x in items if x[1]['features'][0] > 0.99]\n",
    "        if not valid: continue\n",
    "        ##### [REQ-7] Minimality Check: Sort by column length first\n",
    "        valid.sort(key=lambda x: (-x[0], len(x[1]['cols']), -x[1]['features'][7]))\n",
    "        best_prob, best_cand = valid[0]\n",
    "        schema[\"tables\"][table] = [{\n",
    "            \"type\": \"PK\", \"columns\": list(best_cand['cols']),\n",
    "            ##### [REQ-11] Explainability\n",
    "            \"explainability\": {\"confidence\": f\"{best_prob:.2f}\", \"uniqueness\": f\"{best_cand['features'][0]:.1%}\"}\n",
    "        }]\n",
    "\n",
    "    ##### ##### Data Quality Report##########\n",
    "    dq = []\n",
    "    for c in candidates:\n",
    "        if len(c['cols']) == 1:\n",
    "            dq.append({\n",
    "                \"Table\": c['table'], \"Column\": c['cols'][0], \n",
    "                \"Rows\": c.get('n_rows', 0), \n",
    "                \"Uniqueness\": f\"{c['features'][0]:.1%}\", \"Nulls\": f\"{c['features'][1]:.1%}\"\n",
    "            })\n",
    "    pd.DataFrame(dq).to_csv(\"data_quality_profile.csv\", index=False)\n",
    "    print(\"  > Saved: data_quality_profile.csv\")\n",
    "    return schema\n",
    "\n",
    "##### ==========================================###############\n",
    "##### 4. MAIN ORCHESTRATOR              ##########\n",
    "##### ==========================================####################\n",
    "def main():\n",
    "    print(f\"--- Sieve-GNN: Pipeline Start ({CONFIG['num_workers']} Workers) ---\")\n",
    "    files = glob.glob(os.path.join(CONFIG['folder'], \"*.csv\"))\n",
    "    if not files: print(\"No CSVs found.\"); return\n",
    "    \n",
    "    ##### 1. SETTING UP OUR WORKERS ##########\n",
    "    \n",
    "    with open(\"sieve_workers.py\", \"w\") as f: f.write(WORKER_CODE)\n",
    "    spec = importlib.util.spec_from_file_location(\"sieve_workers\", \"sieve_workers.py\")\n",
    "    workers = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"sieve_workers\"] = workers\n",
    "    spec.loader.exec_module(workers)\n",
    "    \n",
    "    print(f\"Phase 1: Profiling {len(files)} tables...\")\n",
    "    all_candidates = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=CONFIG['num_workers']) as executor:\n",
    "        futures = [executor.submit(workers.process_table_file, f) for f in files]\n",
    "        for i, fut in enumerate(as_completed(futures)):\n",
    "            res = fut.result()\n",
    "            if res: all_candidates.extend(res)\n",
    "            if i % 50 == 0: print(f\"  Processed {i}/{len(files)}...\", end=\"\\r\")\n",
    "            \n",
    "    print(f\"\\n  > Candidates: {len(all_candidates)}\")\n",
    "    if len(all_candidates) < 2: return\n",
    "\n",
    "    ### 2. GRAPH & LEARN #####\n",
    "    data = build_graph_fast(all_candidates)\n",
    "    labels, mask = get_weak_labels(all_candidates)\n",
    "    \n",
    "    if mask.sum() > 0:\n",
    "        model = SieveGNN(11, CONFIG['hidden_dim'], CONFIG['heads'], CONFIG['dropout'])\n",
    "        train_model(model, data, labels, mask, CONFIG)\n",
    "        \n",
    "        ##### 3. PREDICT & REFINE #####\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(model(data)).squeeze().numpy()\n",
    "        \n",
    "        initial_schema = generate_reports(all_candidates, probs)\n",
    "        final_schema = refine_and_export(all_candidates, initial_schema, data)\n",
    "        \n",
    "        with open(\"schema_final.json\", \"w\") as f: json.dump(final_schema, f, indent=2)\n",
    "        print(\"  > Saved: schema_final.json\")\n",
    "    else:\n",
    "        print(\"Error: No labels generated.\")\n",
    "\n",
    "##ALLAH BHORSHA\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing\n",
    "    multiprocessing.freeze_support()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa373ad9-82ec-4f26-a2ce-0d15f4b44efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd05ff5-f141-415d-b310-5e673e9d8562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
